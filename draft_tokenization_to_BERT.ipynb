{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d078574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM, AdamW, WarmupLinearSchedule\n",
    "import logging\n",
    "import pandas as pd\n",
    "from biopandas.pdb import PandasPdb\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751542d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"/srv01/technion/morant/Storage/sample_submission.csv\")\n",
    "test = pd.read_csv(\"/srv01/technion/morant/Storage/test.csv\")\n",
    "train_updates = pd.read_csv(\"/srv01/technion/morant/Storage/train_updates_20220929.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d87ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomProteinDataset(Dataset):\n",
    "    def __init__(self, csv_file, wt_struc_pred):\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.wt_struc_pred = PandasPdb().read_pdb(wt_struc_pred)\n",
    "        \n",
    "        # Tokenization of train\n",
    "        aa2num = {'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10, 'L': 11, 'K': 12, 'M': 13,\n",
    "                  'F': 14, 'P': 15, 'O': 16, 'S': 17, 'U': 18, 'T': 19, 'W': 20, 'Y': 21, 'V': 22, 'B': 23, 'Z': 24, 'X': 25, 'J': 26}\n",
    "\n",
    "        # Tokenization!!\n",
    "        self.csv_file['protein_sequence_tokenized'] = self.csv_file['protein_sequence'].apply(lambda s: [aa2num[x] for x in s])\n",
    "        self.csv_file['len_Before_tokenization'] = self.csv_file['protein_sequence'].apply(len)\n",
    "        max_len = self.csv_file['protein_sequence_tokenized'].apply(len).max()\n",
    "\n",
    "        self.csv_file['protein_sequence_tokenized'] = self.csv_file['protein_sequence_tokenized'].apply(\n",
    "            lambda x: np.pad(x, (0, max_len-len(x))))\n",
    "        self.tokens_tensor = self.csv_file['protein_sequence_tokenized']\n",
    "        self.tokens_tensor = torch.tensor(np.array([ x for x in self.tokens_tensor.values ]))[:, :512]\n",
    "        self.tokens_mskd_tensor = self.csv_file['len_Before_tokenization']\n",
    "        self.tokens_mskd_tensor = torch.tensor([np.pad(np.ones(x),(0,max_len-x))\n",
    "                                                for x in self.tokens_mskd_tensor.values])[:, :512].float()\n",
    "        self.tm_tensor = torch.tensor(self.csv_file['tm'])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.tokens_tensor[idx], self.tokens_mskd_tensor[idx],  self.tm_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c48870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3805922/3844659123.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  self.tokens_mskd_tensor = torch.tensor([np.pad(np.ones(x),(0,max_len-x))\n"
     ]
    }
   ],
   "source": [
    "training_data = CustomProteinDataset('/srv01/technion/morant/Storage/train.csv',\n",
    "                                     '/srv01/technion/morant/Storage/wildtype_structure_prediction_af2.pdb')\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e097b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.linear = torch.nn.Sequential(torch.nn.Linear(in_features=393216, out_features=10000),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(in_features=10000, out_features=1000),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(in_features=1000, out_features=1))\n",
    "        \n",
    "    def forward(self, batch, attention_mask):\n",
    "        result = self.model(batch, token_type_ids=torch.zeros_like(batch), attention_mask=attention_mask)[0]\n",
    "        res_flat = torch.flatten(result, start_dim=1)\n",
    "        lin = self.linear(res_flat)\n",
    "\n",
    "        return lin        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf515c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c2435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training (when we'll get there)\n",
    "# Parameters:\n",
    "lr = 1e-5\n",
    "max_grad_norm = 0.7\n",
    "num_total_steps = 1000\n",
    "num_warmup_steps = 500\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
    "\n",
    "### In PyTorch-Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler\n",
    "### and used like this:\n",
    "for i in range(num_total_steps):\n",
    "    for batch,attention_mask, train_tm in train_dataloader:\n",
    "#     for batch,attention_mask, train_tm in zip(batched_tok_ten, batched_tok_mskd, batched_train_tm):\n",
    "        loss_new = loss(model(batch, attention_mask),train_tm.float()[:,None])\n",
    "        loss_new.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f\"loss_new={loss_new}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd5a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (current)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
