{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d078574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn\n",
    "from pytorch_transformers import BertTokenizer, BertModel, BertForMaskedLM, AdamW, WarmupLinearSchedule\n",
    "import logging\n",
    "import pandas as pd\n",
    "from biopandas.pdb import PandasPdb\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba8d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"/srv01/technion/morant/Storage/sample_submission.csv\")\n",
    "test = pd.read_csv(\"/srv01/technion/morant/Storage/test.csv\")\n",
    "train_updates = pd.read_csv(\"/srv01/technion/morant/Storage/train_updates_20220929.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda011c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomProteinDataset(Dataset):\n",
    "    def __init__(self, csv_file, wt_struc_pred):\n",
    "        self.csv_file = pd.read_csv(csv_file)\n",
    "        self.wt_struc_pred = PandasPdb().read_pdb(wt_struc_pred)\n",
    "        \n",
    "        # Tokenization of train\n",
    "        aa2num = {'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10, 'L': 11, 'K': 12, 'M': 13,\n",
    "                  'F': 14, 'P': 15, 'O': 16, 'S': 17, 'U': 18, 'T': 19, 'W': 20, 'Y': 21, 'V': 22, 'B': 23, 'Z': 24, 'X': 25, 'J': 26}\n",
    "\n",
    "        # Tokenization!!\n",
    "        self.csv_file['protein_sequence_tokenized'] = self.csv_file['protein_sequence'].apply(lambda s: [aa2num[x] for x in s])\n",
    "        self.csv_file['len_Before_tokenization'] = self.csv_file['protein_sequence'].apply(len)\n",
    "        self.csv_file = self.csv_file[self.csv_file['len_Before_tokenization']<=512].reset_index()\n",
    "        max_len = self.csv_file['protein_sequence_tokenized'].apply(len).max()\n",
    "\n",
    "        self.csv_file['protein_sequence_tokenized'] = self.csv_file['protein_sequence_tokenized'].apply(\n",
    "            lambda x: np.pad(x, (0, max_len-len(x))))\n",
    "        self.tokens_tensor = self.csv_file['protein_sequence_tokenized']\n",
    "        self.tokens_tensor = torch.tensor(np.array([ x for x in self.tokens_tensor.values ]))[:, :512]\n",
    "        self.tokens_mskd_tensor = self.csv_file['len_Before_tokenization']\n",
    "        self.tokens_mskd_tensor = torch.tensor([np.pad(np.ones(x),(0,max_len-x))\n",
    "                                                for x in self.tokens_mskd_tensor.values])[:, :512].float()\n",
    "        self.tm_tensor = torch.tensor(self.csv_file['tm'])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.tokens_tensor[idx], self.tokens_mskd_tensor[idx],  self.tm_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1835c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_712435/310424231.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  self.tokens_mskd_tensor = torch.tensor([np.pad(np.ones(x),(0,max_len-x))\n"
     ]
    }
   ],
   "source": [
    "training_data = CustomProteinDataset('/srv01/technion/morant/Storage/train.csv',\n",
    "                                     '/srv01/technion/morant/Storage/wildtype_structure_prediction_af2.pdb')\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e097b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.linear = torch.nn.Sequential(torch.nn.Linear(in_features=393216, out_features=20000),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(in_features=20000, out_features=10000),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(in_features=10000, out_features=1000),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(in_features=1000, out_features=1))\n",
    "        \n",
    "    def forward(self, batch, attention_mask):\n",
    "        result = self.model(batch, token_type_ids=torch.zeros_like(batch), attention_mask=attention_mask)[0]\n",
    "        res_flat = torch.flatten(result, start_dim=1)\n",
    "        lin = self.linear(res_flat)\n",
    "\n",
    "        return lin        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf515c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c2c2435",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_new=326.58544921875\n",
      "loss_new=349.9559326171875\n",
      "loss_new=289.32427978515625\n",
      "loss_new=243.9564666748047\n",
      "loss_new=499.64599609375\n",
      "loss_new=340.3167419433594\n",
      "loss_new=390.4734191894531\n",
      "loss_new=339.09759521484375\n",
      "loss_new=309.1086730957031\n",
      "loss_new=326.2749938964844\n",
      "loss_new=289.9901123046875\n",
      "loss_new=272.30377197265625\n",
      "loss_new=253.0203857421875\n",
      "loss_new=381.80572509765625\n",
      "loss_new=397.52154541015625\n",
      "loss_new=370.40472412109375\n",
      "loss_new=305.77520751953125\n",
      "loss_new=191.43597412109375\n",
      "loss_new=369.45086669921875\n",
      "loss_new=448.6501159667969\n",
      "loss_new=419.50274658203125\n",
      "loss_new=239.0080108642578\n",
      "loss_new=338.41046142578125\n",
      "loss_new=457.9380798339844\n",
      "loss_new=516.4689331054688\n",
      "loss_new=346.7664794921875\n",
      "loss_new=309.12725830078125\n",
      "loss_new=323.1715087890625\n",
      "loss_new=325.1849670410156\n",
      "loss_new=253.74864196777344\n",
      "loss_new=316.572021484375\n",
      "loss_new=314.2746887207031\n",
      "loss_new=325.5556945800781\n",
      "loss_new=428.58306884765625\n",
      "loss_new=292.0516357421875\n",
      "loss_new=350.1146545410156\n",
      "loss_new=280.99200439453125\n",
      "loss_new=347.4201354980469\n",
      "loss_new=377.898193359375\n",
      "loss_new=378.41448974609375\n",
      "loss_new=297.3006591796875\n",
      "loss_new=221.96083068847656\n",
      "loss_new=322.9389953613281\n",
      "loss_new=307.5564880371094\n",
      "loss_new=184.526123046875\n",
      "loss_new=303.58056640625\n",
      "loss_new=183.0189208984375\n",
      "loss_new=300.1689758300781\n",
      "loss_new=322.15106201171875\n",
      "loss_new=434.2880859375\n",
      "loss_new=293.88800048828125\n",
      "loss_new=263.48187255859375\n",
      "loss_new=309.7296447753906\n",
      "loss_new=265.1645202636719\n",
      "loss_new=273.3505859375\n",
      "loss_new=284.25067138671875\n",
      "loss_new=307.29022216796875\n",
      "loss_new=306.99945068359375\n",
      "loss_new=313.0472717285156\n",
      "loss_new=479.3998107910156\n",
      "loss_new=319.085693359375\n",
      "loss_new=218.45921325683594\n",
      "loss_new=350.1374206542969\n",
      "loss_new=247.7835235595703\n",
      "loss_new=264.508544921875\n",
      "loss_new=298.9013671875\n",
      "loss_new=222.20050048828125\n",
      "loss_new=260.76318359375\n",
      "loss_new=273.1841125488281\n",
      "loss_new=223.59271240234375\n",
      "loss_new=304.8495178222656\n",
      "loss_new=234.41357421875\n",
      "loss_new=230.38758850097656\n",
      "loss_new=255.43540954589844\n",
      "loss_new=232.15524291992188\n",
      "loss_new=320.0368347167969\n",
      "loss_new=344.0818786621094\n",
      "loss_new=206.73023986816406\n",
      "loss_new=282.5899658203125\n",
      "loss_new=373.75482177734375\n",
      "loss_new=314.1351623535156\n",
      "loss_new=275.3224182128906\n",
      "loss_new=237.02134704589844\n",
      "loss_new=282.011962890625\n",
      "loss_new=289.3185729980469\n",
      "loss_new=407.4558410644531\n",
      "loss_new=183.8699493408203\n",
      "loss_new=275.6504821777344\n",
      "loss_new=260.86163330078125\n",
      "loss_new=162.74586486816406\n",
      "loss_new=288.3503112792969\n",
      "loss_new=239.59019470214844\n",
      "loss_new=207.2147216796875\n",
      "loss_new=202.19766235351562\n",
      "loss_new=231.37669372558594\n",
      "loss_new=189.1962890625\n",
      "loss_new=282.98046875\n",
      "loss_new=181.04588317871094\n",
      "loss_new=255.52203369140625\n",
      "loss_new=211.720703125\n",
      "loss_new=289.3529052734375\n",
      "loss_new=219.84568786621094\n",
      "loss_new=297.98992919921875\n",
      "loss_new=230.57156372070312\n",
      "loss_new=292.01739501953125\n",
      "loss_new=205.69972229003906\n",
      "loss_new=240.74559020996094\n",
      "loss_new=318.496826171875\n",
      "loss_new=225.32603454589844\n",
      "loss_new=274.4418640136719\n",
      "loss_new=196.5635986328125\n",
      "loss_new=234.81253051757812\n",
      "loss_new=109.51703643798828\n",
      "loss_new=165.58279418945312\n",
      "loss_new=237.41830444335938\n",
      "loss_new=223.35760498046875\n",
      "loss_new=159.43788146972656\n",
      "loss_new=239.37008666992188\n",
      "loss_new=195.26364135742188\n",
      "loss_new=292.9533996582031\n",
      "loss_new=173.04269409179688\n",
      "loss_new=231.6416015625\n",
      "loss_new=215.9506072998047\n",
      "loss_new=286.74169921875\n",
      "loss_new=238.3026580810547\n",
      "loss_new=285.75335693359375\n",
      "loss_new=255.57272338867188\n",
      "loss_new=284.1259460449219\n",
      "loss_new=167.3861846923828\n",
      "loss_new=284.04412841796875\n",
      "loss_new=295.27984619140625\n",
      "loss_new=215.5991668701172\n",
      "loss_new=190.05682373046875\n",
      "loss_new=287.2562255859375\n",
      "loss_new=234.34690856933594\n",
      "loss_new=244.96424865722656\n",
      "loss_new=234.4557647705078\n",
      "loss_new=148.43064880371094\n",
      "loss_new=256.6282958984375\n",
      "loss_new=227.8929443359375\n",
      "loss_new=252.897705078125\n",
      "loss_new=217.16189575195312\n",
      "loss_new=219.33721923828125\n",
      "loss_new=247.11273193359375\n",
      "loss_new=202.106201171875\n",
      "loss_new=191.3141632080078\n",
      "loss_new=195.84677124023438\n",
      "loss_new=251.33145141601562\n",
      "loss_new=178.25509643554688\n",
      "loss_new=135.70068359375\n",
      "loss_new=191.88250732421875\n",
      "loss_new=254.87002563476562\n",
      "loss_new=222.70321655273438\n",
      "loss_new=289.5224914550781\n",
      "loss_new=174.2971954345703\n",
      "loss_new=215.38404846191406\n",
      "loss_new=231.9189453125\n",
      "loss_new=254.1273956298828\n",
      "loss_new=293.60894775390625\n",
      "loss_new=191.31727600097656\n",
      "loss_new=284.5277099609375\n",
      "loss_new=315.4535827636719\n",
      "loss_new=259.3960266113281\n",
      "loss_new=128.02029418945312\n",
      "loss_new=234.95762634277344\n",
      "loss_new=200.8638916015625\n",
      "loss_new=230.22439575195312\n",
      "loss_new=193.5799560546875\n",
      "loss_new=269.7975769042969\n",
      "loss_new=247.97238159179688\n",
      "loss_new=221.99411010742188\n",
      "loss_new=259.476806640625\n",
      "loss_new=385.3948669433594\n",
      "loss_new=285.5633850097656\n",
      "loss_new=249.22015380859375\n",
      "loss_new=288.9813537597656\n",
      "loss_new=191.70082092285156\n",
      "loss_new=172.77145385742188\n",
      "loss_new=180.552978515625\n",
      "loss_new=237.7296142578125\n",
      "loss_new=214.0185546875\n",
      "loss_new=229.48190307617188\n",
      "loss_new=258.5853576660156\n",
      "loss_new=186.43887329101562\n",
      "loss_new=167.20657348632812\n",
      "loss_new=179.22576904296875\n",
      "loss_new=311.05316162109375\n",
      "loss_new=280.51934814453125\n",
      "loss_new=200.4841766357422\n",
      "loss_new=308.650146484375\n",
      "loss_new=249.31764221191406\n",
      "loss_new=186.16329956054688\n",
      "loss_new=175.99195861816406\n",
      "loss_new=271.87213134765625\n",
      "loss_new=340.14141845703125\n",
      "loss_new=348.6993408203125\n",
      "loss_new=297.5831604003906\n",
      "loss_new=143.59483337402344\n",
      "loss_new=191.61679077148438\n",
      "loss_new=164.5991973876953\n",
      "loss_new=201.73773193359375\n",
      "loss_new=138.52951049804688\n",
      "loss_new=172.25515747070312\n",
      "loss_new=207.8690643310547\n",
      "loss_new=192.3402099609375\n",
      "loss_new=271.0564880371094\n",
      "loss_new=165.74188232421875\n",
      "loss_new=221.7405548095703\n",
      "loss_new=289.8782958984375\n",
      "loss_new=246.66445922851562\n",
      "loss_new=210.6614227294922\n",
      "loss_new=273.8280944824219\n",
      "loss_new=211.04672241210938\n",
      "loss_new=171.45120239257812\n",
      "loss_new=304.8701171875\n",
      "loss_new=290.3755798339844\n",
      "loss_new=197.763671875\n",
      "loss_new=194.82180786132812\n",
      "loss_new=212.92813110351562\n",
      "loss_new=153.85975646972656\n",
      "loss_new=209.5502166748047\n",
      "loss_new=176.65135192871094\n",
      "loss_new=272.0179443359375\n",
      "loss_new=176.20822143554688\n",
      "loss_new=243.17861938476562\n",
      "loss_new=236.77806091308594\n",
      "loss_new=211.0385284423828\n",
      "loss_new=237.08987426757812\n",
      "loss_new=246.5295867919922\n",
      "loss_new=283.61669921875\n",
      "loss_new=151.01292419433594\n",
      "loss_new=264.41815185546875\n",
      "loss_new=222.90032958984375\n",
      "loss_new=175.64907836914062\n",
      "loss_new=182.6818389892578\n",
      "loss_new=226.04420471191406\n",
      "loss_new=197.89761352539062\n",
      "loss_new=220.376220703125\n",
      "loss_new=197.5220947265625\n",
      "loss_new=336.0630798339844\n",
      "loss_new=207.1857452392578\n",
      "loss_new=223.9141387939453\n",
      "loss_new=178.29727172851562\n",
      "loss_new=229.118896484375\n",
      "loss_new=224.5379638671875\n",
      "loss_new=247.5860595703125\n",
      "loss_new=204.50247192382812\n",
      "loss_new=249.70028686523438\n",
      "loss_new=205.335693359375\n",
      "loss_new=253.10824584960938\n",
      "loss_new=111.98960876464844\n",
      "loss_new=220.06983947753906\n",
      "loss_new=255.1273956298828\n",
      "loss_new=236.7020263671875\n",
      "loss_new=206.8295135498047\n",
      "loss_new=229.1774139404297\n",
      "loss_new=262.17724609375\n",
      "loss_new=196.0767059326172\n",
      "loss_new=235.40890502929688\n",
      "loss_new=199.20083618164062\n",
      "loss_new=194.26132202148438\n",
      "loss_new=216.147705078125\n",
      "loss_new=199.48687744140625\n",
      "loss_new=229.5193328857422\n",
      "loss_new=199.73101806640625\n",
      "loss_new=255.2777099609375\n",
      "loss_new=181.88478088378906\n",
      "loss_new=224.09671020507812\n",
      "loss_new=204.41680908203125\n",
      "loss_new=281.87060546875\n",
      "loss_new=245.7349853515625\n",
      "loss_new=285.5914306640625\n",
      "loss_new=259.0469665527344\n",
      "loss_new=222.60696411132812\n",
      "loss_new=306.7768249511719\n",
      "loss_new=177.63441467285156\n",
      "loss_new=228.2561798095703\n",
      "loss_new=238.4176788330078\n",
      "loss_new=182.15924072265625\n",
      "loss_new=269.6215515136719\n",
      "loss_new=201.15391540527344\n",
      "loss_new=194.60816955566406\n",
      "loss_new=200.47946166992188\n",
      "loss_new=183.5480194091797\n",
      "loss_new=279.56671142578125\n",
      "loss_new=252.54359436035156\n",
      "loss_new=289.88330078125\n",
      "loss_new=256.33306884765625\n",
      "loss_new=232.1116943359375\n",
      "loss_new=246.85629272460938\n",
      "loss_new=166.10157775878906\n",
      "loss_new=341.3894348144531\n",
      "loss_new=242.59579467773438\n",
      "loss_new=326.56817626953125\n",
      "loss_new=201.21102905273438\n",
      "loss_new=270.1141357421875\n",
      "loss_new=284.527099609375\n",
      "loss_new=229.89236450195312\n",
      "loss_new=210.4571533203125\n",
      "loss_new=213.29144287109375\n",
      "loss_new=182.6601104736328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_new=315.3497619628906\n",
      "loss_new=274.94500732421875\n",
      "loss_new=278.5273132324219\n",
      "loss_new=202.3355712890625\n",
      "loss_new=269.4154052734375\n",
      "loss_new=191.351318359375\n",
      "loss_new=208.30316162109375\n",
      "loss_new=333.724609375\n",
      "loss_new=279.8355712890625\n",
      "loss_new=270.4465637207031\n",
      "loss_new=199.1775360107422\n",
      "loss_new=192.83511352539062\n",
      "loss_new=241.1897735595703\n",
      "loss_new=201.81417846679688\n",
      "loss_new=141.26068115234375\n",
      "loss_new=147.46449279785156\n",
      "loss_new=225.9024658203125\n",
      "loss_new=120.71314239501953\n",
      "loss_new=180.33358764648438\n",
      "loss_new=119.72064971923828\n",
      "loss_new=227.5009002685547\n",
      "loss_new=264.8646240234375\n",
      "loss_new=273.8433837890625\n",
      "loss_new=222.6355438232422\n",
      "loss_new=262.9760437011719\n",
      "loss_new=127.4345703125\n",
      "loss_new=289.230712890625\n",
      "loss_new=222.5192413330078\n",
      "loss_new=103.15982055664062\n",
      "loss_new=317.38214111328125\n",
      "loss_new=203.0672149658203\n",
      "loss_new=238.67633056640625\n",
      "loss_new=258.5260009765625\n",
      "loss_new=291.3746643066406\n",
      "loss_new=243.1175537109375\n",
      "loss_new=249.37588500976562\n",
      "loss_new=254.93206787109375\n",
      "loss_new=238.1466064453125\n",
      "loss_new=148.1869659423828\n",
      "loss_new=163.54156494140625\n",
      "loss_new=285.77197265625\n",
      "loss_new=186.59634399414062\n",
      "loss_new=207.01046752929688\n",
      "loss_new=240.54803466796875\n",
      "loss_new=193.9744873046875\n",
      "loss_new=177.86123657226562\n",
      "loss_new=303.162109375\n",
      "loss_new=391.89019775390625\n",
      "loss_new=239.757568359375\n",
      "loss_new=218.18836975097656\n",
      "loss_new=256.0954895019531\n",
      "loss_new=137.65814208984375\n",
      "loss_new=246.37728881835938\n",
      "loss_new=239.1844482421875\n",
      "loss_new=215.44882202148438\n",
      "loss_new=308.0220642089844\n",
      "loss_new=257.88299560546875\n",
      "loss_new=275.47705078125\n",
      "loss_new=163.84136962890625\n",
      "loss_new=267.4112854003906\n",
      "loss_new=214.7306671142578\n",
      "loss_new=226.10377502441406\n",
      "loss_new=226.11590576171875\n",
      "loss_new=210.64889526367188\n",
      "loss_new=288.404052734375\n",
      "loss_new=218.88018798828125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_total_steps):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch,attention_mask, train_tm \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     for batch,attention_mask, train_tm in zip(batched_tok_ten, batched_tok_mskd, batched_train_tm):\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         loss_new \u001b[38;5;241m=\u001b[39m loss(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m,train_tm\u001b[38;5;241m.\u001b[39mfloat()[:,\u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m     17\u001b[0m         loss_new\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)  \u001b[38;5;66;03m# Gradient clipping is not in AdamW anymore (so you can use amp without issue)\u001b[39;00m\n",
      "File \u001b[0;32m/Local/md_kaplan/anaconda3/envs/current/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, batch, attention_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch, token_type_ids\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros_like(batch), attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m res_flat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(result, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m lin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lin\n",
      "File \u001b[0;32m/Local/md_kaplan/anaconda3/envs/current/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Local/md_kaplan/anaconda3/envs/current/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Local/md_kaplan/anaconda3/envs/current/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Local/md_kaplan/anaconda3/envs/current/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training (when we'll get there)\n",
    "# Parameters:\n",
    "lr = 1e-7\n",
    "max_grad_norm = 0.7\n",
    "num_total_steps = 1000\n",
    "num_warmup_steps = 500\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
    "\n",
    "### In PyTorch-Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler\n",
    "### and used like this:\n",
    "for i in range(num_total_steps):\n",
    "    for batch,attention_mask, train_tm in train_dataloader:\n",
    "#     for batch,attention_mask, train_tm in zip(batched_tok_ten, batched_tok_mskd, batched_train_tm):\n",
    "        loss_new = loss(model(batch, attention_mask),train_tm.float()[:,None])\n",
    "        loss_new.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        print(f\"loss_new={loss_new}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc4143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf980660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fcd7f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'SHELL': '/bin/bash',\n",
       "        'JPY_API_TOKEN': 'b2c0e785b69842beb3fd9231d0290cb6',\n",
       "        'USER': 'morant',\n",
       "        'JUPYTERHUB_BASE_URL': '/',\n",
       "        'JUPYTERHUB_CLIENT_ID': 'jupyterhub-user-morant',\n",
       "        'JUPYTERHUB_API_TOKEN': 'b2c0e785b69842beb3fd9231d0290cb6',\n",
       "        'PATH': '/Local/md_kaplan/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin',\n",
       "        'MKL_NUM_THREADS': '3',\n",
       "        'PWD': '/srv01/technion/morant',\n",
       "        'JUPYTERHUB_SERVER_NAME': '',\n",
       "        'LANG': 'en_US.UTF-8',\n",
       "        'JUPYTERHUB_API_URL': 'http://127.0.0.1:8002/hub/api',\n",
       "        'SHLVL': '0',\n",
       "        'HOME': '/srv01/technion/morant',\n",
       "        'JUPYTERHUB_USER': 'morant',\n",
       "        'JUPYTERHUB_ACTIVITY_URL': 'http://127.0.0.1:8002/hub/api/users/morant/activity',\n",
       "        'JUPYTERHUB_OAUTH_CALLBACK_URL': '/user/morant/oauth_callback',\n",
       "        'JUPYTERHUB_HOST': '',\n",
       "        'JUPYTERHUB_SERVICE_PREFIX': '/user/morant/',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_PARENT_PID': '3785150',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       "        'KMP_INIT_AT_FORK': 'FALSE'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06389baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (current)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
